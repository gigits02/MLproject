\relax 
\@writefile{toc}{\contentsline {section}{\numberline {0.1}ML purposes}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Data}{7}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Encoding}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Further terminologies}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Tasks}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Supervised Learning}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Unsupervised Learning}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Classification}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Regression}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Models}{9}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning Algorithm}{11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The role of the inductive bias}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Find the Version Space (VS)}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Unbiased Learner}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Tasks and Loss}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Regression}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Classification}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Generalization}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Validation}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Generalization issues}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Complexity control}{15}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Complexity on a case of study}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Formal setting}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Vapnik-Chervonenkis-dim and Statistical Learning Theory (SLT)}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Validation}{17}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Two aims}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Validation: ideal world}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Hold out cross validation}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}K-fold cross validation}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Classification accuracy}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Confusion matrix}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Linear Models}{21}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Regression}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Solve it}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Notation for multidimensional inputs}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Classification}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Learning algorithms}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Normal equation algorithm}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computing the pseudoinverse of X}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Gradient descent}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch/On-line version of gradient descent}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Linear models: inductive bias}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Extending linear model: LBE}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Improvements: Tikhonov regularization}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Solving it}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Tikhonov regularization: trade-off}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}KNN}{25}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}1-nn}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}K-nn}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Limits of K-nn}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Bias/Variance and ensembling}{27}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Bias-Var. Analysis}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Bias/Var and regularization}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Ensemble Learning}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Bagging (bootstrap aggregating)}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Boosting (e.g. AdaBoost)}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Networks models}{31}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Artificial neuron: processing unit}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Perceptron}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}McCulloch and Pitts networks}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Properties of Networks of perceptrons}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Learning for one unit model - Overview}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.5}The perceptron learning algorithm}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.6}Perceptron's Convergence Theorem}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.7}Perceptron Learn. Alg. Vs LMS alg.}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Activation functions}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Activation functions: derivatives}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Neural networks (NN)}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}The Multi Layer Perceptron (MLP)}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Feedforward processing}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}NN flexibility: which tasks?}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}NN as a function}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NN compared to LBE}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Universal approximation}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.4}The learning algorithms: back-propagation}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Iterative gradient descent training algorithm}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Case 1: Output Unit (t=k)}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Case 2: Hidden unit (t=j)}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Learning rate - improvements}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Nesterov Momentum}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Support Vector Machines (SVM)}{43}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}SVM for binary classification}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Linear separable patterns - Hard margin SVM}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Separation margin}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimal hyperplane}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Canonical representation of the hyperplane}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Support Vector}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computing the distance}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Quadratic optimization problem}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimization problem - Primal form}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solving the quadratic problem}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kuhn-Tucker conditions}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimization problem - Dual form}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solving dual problem}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Soft margin SVM}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Support vector (soft margin)}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Primal problem - soft margin}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dual problem (soft margin)}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kuhn-Tucker conditions (soft margin)}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solving the problem}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Mapping to High-Dimensional feature space}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solving the problem in the Feature Space}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kernel trick}{50}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Properties of Kernels}{50}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Primal problem in feature space}{51}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dual problem in feature space}{51}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Some examples of Kernels}{51}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.2}SVM for non-linear regression}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}$\epsilon $-insensitive (soft margin) loss}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Formulating the optimization problem}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Primal problem}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.4}Dual problem}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.5}Computing the weight vector}{54}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.6}Computing the estimate}{54}{}\protected@file@percent }
\gdef \@abspage@last{54}
