\contentsline {section}{\numberline {0.1}ML purposes}{5}{}%
\contentsline {chapter}{\numberline {1}Data}{7}{}%
\contentsline {section}{\numberline {1.1}Encoding}{7}{}%
\contentsline {section}{\numberline {1.2}Further terminologies}{7}{}%
\contentsline {section}{\numberline {1.3}Tasks}{7}{}%
\contentsline {subsection}{\numberline {1.3.1}Supervised Learning}{7}{}%
\contentsline {subsection}{\numberline {1.3.2}Unsupervised Learning}{8}{}%
\contentsline {subsection}{\numberline {1.3.3}Classification}{8}{}%
\contentsline {subsection}{\numberline {1.3.4}Regression}{8}{}%
\contentsline {chapter}{\numberline {2}Models}{9}{}%
\contentsline {chapter}{\numberline {3}Learning Algorithm}{11}{}%
\contentsline {section}{\numberline {3.1}The role of the inductive bias}{11}{}%
\contentsline {section}{\numberline {3.2}Find the Version Space (VS)}{11}{}%
\contentsline {section}{\numberline {3.3}Unbiased Learner}{11}{}%
\contentsline {section}{\numberline {3.4}Tasks and Loss}{12}{}%
\contentsline {subsection}{\numberline {3.4.1}Regression}{12}{}%
\contentsline {subsection}{\numberline {3.4.2}Classification}{12}{}%
\contentsline {section}{\numberline {3.5}Generalization}{13}{}%
\contentsline {section}{\numberline {3.6}Validation}{13}{}%
\contentsline {section}{\numberline {3.7}Generalization issues}{13}{}%
\contentsline {chapter}{\numberline {4}Complexity control}{15}{}%
\contentsline {section}{\numberline {4.1}Complexity on a case of study}{15}{}%
\contentsline {section}{\numberline {4.2}Formal setting}{16}{}%
\contentsline {section}{\numberline {4.3}Vapnik-Chervonenkis-dim and Statistical Learning Theory (SLT)}{16}{}%
\contentsline {chapter}{\numberline {5}Validation}{17}{}%
\contentsline {section}{\numberline {5.1}Two aims}{17}{}%
\contentsline {section}{\numberline {5.2}Validation: ideal world}{17}{}%
\contentsline {section}{\numberline {5.3}Hold out cross validation}{17}{}%
\contentsline {section}{\numberline {5.4}K-fold cross validation}{18}{}%
\contentsline {section}{\numberline {5.5}Classification accuracy}{19}{}%
\contentsline {subsection}{\numberline {5.5.1}Confusion matrix}{19}{}%
\contentsline {chapter}{\numberline {6}Linear Models}{21}{}%
\contentsline {section}{\numberline {6.1}Regression}{21}{}%
\contentsline {subsection}{\numberline {6.1.1}Solve it}{21}{}%
\contentsline {subsection}{\numberline {6.1.2}Notation for multidimensional inputs}{21}{}%
\contentsline {section}{\numberline {6.2}Classification}{22}{}%
\contentsline {section}{\numberline {6.3}Learning algorithms}{22}{}%
\contentsline {subsection}{\numberline {6.3.1}Normal equation algorithm}{22}{}%
\contentsline {subsubsection}{Computing the pseudoinverse of X}{22}{}%
\contentsline {subsection}{\numberline {6.3.2}Gradient descent}{23}{}%
\contentsline {subsubsection}{Batch/On-line version of gradient descent}{23}{}%
\contentsline {section}{\numberline {6.4}Linear models: inductive bias}{23}{}%
\contentsline {section}{\numberline {6.5}Extending linear model: LBE}{24}{}%
\contentsline {section}{\numberline {6.6}Improvements: Tikhonov regularization}{24}{}%
\contentsline {subsection}{\numberline {6.6.1}Solving it}{24}{}%
\contentsline {subsection}{\numberline {6.6.2}Tikhonov regularization: trade-off}{24}{}%
\contentsline {chapter}{\numberline {7}KNN}{25}{}%
\contentsline {section}{\numberline {7.1}1-nn}{25}{}%
\contentsline {section}{\numberline {7.2}K-nn}{25}{}%
\contentsline {section}{\numberline {7.3}Limits of K-nn}{26}{}%
\contentsline {chapter}{\numberline {8}Bias/Variance and ensembling}{27}{}%
\contentsline {section}{\numberline {8.1}Bias-Var. Analysis}{27}{}%
\contentsline {section}{\numberline {8.2}Bias/Var and regularization}{28}{}%
\contentsline {section}{\numberline {8.3}Ensemble Learning}{29}{}%
\contentsline {subsection}{\numberline {8.3.1}Bagging (bootstrap aggregating)}{29}{}%
\contentsline {subsection}{\numberline {8.3.2}Boosting (e.g. AdaBoost)}{29}{}%
\contentsline {chapter}{\numberline {9}Networks models}{31}{}%
\contentsline {section}{\numberline {9.1}Artificial neuron: processing unit}{31}{}%
\contentsline {subsection}{\numberline {9.1.1}Perceptron}{31}{}%
\contentsline {subsection}{\numberline {9.1.2}McCulloch and Pitts networks}{32}{}%
\contentsline {subsection}{\numberline {9.1.3}Properties of Networks of perceptrons}{32}{}%
\contentsline {subsection}{\numberline {9.1.4}Learning for one unit model - Overview}{32}{}%
\contentsline {subsection}{\numberline {9.1.5}The perceptron learning algorithm}{32}{}%
\contentsline {subsection}{\numberline {9.1.6}Perceptron's Convergence Theorem}{33}{}%
\contentsline {subsection}{\numberline {9.1.7}Perceptron Learn. Alg. Vs LMS alg.}{33}{}%
\contentsline {section}{\numberline {9.2}Activation functions}{33}{}%
\contentsline {subsection}{\numberline {9.2.1}Activation functions: derivatives}{36}{}%
\contentsline {section}{\numberline {9.3}Neural networks (NN)}{36}{}%
\contentsline {subsection}{\numberline {9.3.1}The Multi Layer Perceptron (MLP)}{37}{}%
\contentsline {subsection}{\numberline {9.3.2}Feedforward processing}{37}{}%
\contentsline {subsection}{\numberline {9.3.3}NN flexibility: which tasks?}{37}{}%
\contentsline {subsection}{\numberline {9.3.4}NN as a function}{37}{}%
\contentsline {subsubsection}{NN compared to LBE}{38}{}%
\contentsline {subsection}{\numberline {9.3.5}Universal approximation}{38}{}%
\contentsline {section}{\numberline {9.4}The learning algorithms: back-propagation}{38}{}%
\contentsline {subsection}{\numberline {9.4.1}Iterative gradient descent training algorithm}{39}{}%
\contentsline {subsubsection}{Case 1: Output Unit (t=k)}{40}{}%
\contentsline {subsubsection}{Case 2: Hidden unit (t=j)}{40}{}%
\contentsline {section}{\numberline {9.5}Learning rate - improvements}{41}{}%
\contentsline {subsection}{\numberline {9.5.1}Nesterov Momentum}{41}{}%
\contentsline {chapter}{\numberline {10}Support Vector Machines (SVM)}{43}{}%
\contentsline {section}{\numberline {10.1}SVM for binary classification}{43}{}%
\contentsline {subsection}{\numberline {10.1.1}Linear separable patterns - Hard margin SVM}{43}{}%
\contentsline {subsubsection}{Separation margin}{43}{}%
\contentsline {subsubsection}{Optimal hyperplane}{44}{}%
\contentsline {subsubsection}{Canonical representation of the hyperplane}{44}{}%
\contentsline {subsubsection}{Support Vector}{44}{}%
\contentsline {subsubsection}{Computing the distance}{44}{}%
\contentsline {subsubsection}{Quadratic optimization problem}{45}{}%
\contentsline {subsubsection}{Optimization problem - Primal form}{45}{}%
\contentsline {subsubsection}{Solving the quadratic problem}{46}{}%
\contentsline {subsubsection}{Kuhn-Tucker conditions}{46}{}%
\contentsline {subsubsection}{Optimization problem - Dual form}{46}{}%
\contentsline {subsubsection}{Solving dual problem}{47}{}%
\contentsline {subsection}{\numberline {10.1.2}Soft margin SVM}{47}{}%
\contentsline {subsubsection}{Support vector (soft margin)}{47}{}%
\contentsline {subsubsection}{Primal problem - soft margin}{48}{}%
\contentsline {subsubsection}{Dual problem (soft margin)}{48}{}%
\contentsline {subsubsection}{Kuhn-Tucker conditions (soft margin)}{48}{}%
\contentsline {subsubsection}{Solving the problem}{49}{}%
\contentsline {subsection}{\numberline {10.1.3}Mapping to High-Dimensional feature space}{49}{}%
\contentsline {subsubsection}{Solving the problem in the Feature Space}{49}{}%
\contentsline {subsubsection}{Kernel trick}{50}{}%
\contentsline {subsubsection}{Properties of Kernels}{50}{}%
\contentsline {subsubsection}{Primal problem in feature space}{51}{}%
\contentsline {subsubsection}{Dual problem in feature space}{51}{}%
\contentsline {subsubsection}{Some examples of Kernels}{51}{}%
\contentsline {section}{\numberline {10.2}SVM for non-linear regression}{52}{}%
\contentsline {subsection}{\numberline {10.2.1}$\epsilon $-insensitive (soft margin) loss}{52}{}%
\contentsline {subsection}{\numberline {10.2.2}Formulating the optimization problem}{52}{}%
\contentsline {subsection}{\numberline {10.2.3}Primal problem}{53}{}%
\contentsline {subsection}{\numberline {10.2.4}Dual problem}{53}{}%
\contentsline {subsection}{\numberline {10.2.5}Computing the weight vector}{54}{}%
\contentsline {subsection}{\numberline {10.2.6}Computing the estimate}{54}{}%
