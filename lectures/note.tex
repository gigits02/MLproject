\documentclass[a4paper]{book}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fancybox}
\usepackage{siunitx}

\title{Machine Learning notes}
\author{Luigi Tarasi} 
\date{\today}  

\begin{document}

\maketitle 
\tableofcontents

\section{ML purposes}
The ML studies and proposes methods to build (infer) dependencies/functions/hypothesis from examples of observed data:
\begin{itemize}
    \item that fits the known examples;
    \item able to generalize, with reasonable accuracy for new data
    \item considering the expressiveness and algorithmic compLexity of the models and learning algorithms
\end{itemize}

\chapter{Data}
\section{Encoding}
Flat case:
\begin{itemize}
    \item Numerical encoding for categories: e.g. 0/1 or -1/+1 for 2 classes (or for multi-classes the one-hot-encoding);
    \item Structured: sequences (lists), trees, graphs, tables...
\end{itemize}

\section{Further terminologies}
\begin{itemize}
    \item Noise: addition of external factors to the stream of (target) information (signal); due to randomness in the measurements, not due to the underlying law. (E.g. Gaussian noise)
    \item Outliers: unusual data values that are not consistent with most observations (e.g. due to abnormal measurements errors). Removing in preprocessing.
    \item Feature selection: selections of a small number of informative features. It can provide an optimal input representation for a learning problem.
\end{itemize}

\section{Tasks}
The task defines the purpose of the application:
\begin{itemize}
    \item Predictive: function approximation (classification, regression).
    \item Descriptive: find subsets or groups of unclassified data (clusyer analysis, association rules)
\end{itemize}

\subsection{Supervised Learning}
\textbf{Given}: Training examples as $<input,output>=<\vec{x},d>$ (labelled examples) for an unkown function $f$ known only at the given points of examples

\textbf{Find}: a \textit{good} approximation of $f$ (a hypothesis $h$ that can be used for prediction on unseen data $\vec{x}'$, i.e. that is able to generalize)

\textit{Target}: $d$ (or $t$ or $y$) a categorical or numerical label;
\begin{itemize}
    \item Classification: discrete value outputs:
    $$f(\vec{x}) \in \left\{1,2,...,K\right\} \ \text{classes}$$
    \item Regression: real continuous output values (approximate a real-valued target function, in $R$ or $R^k$)
\end{itemize}
\subsection{Unsupervised Learning}
No teacher!
\begin{itemize}
    \item TR (training set) = set of unlabelled data $<\vec{x}>$
    \item Finding natural groupings in a set of data (clustering, dimensionality reduction / visualization / preprocessing, modelling the data density)
\end{itemize}

\subsection{Classification}
Patterns (feature vectors) are seen as members of a class and the goal is to assing the patterns observed classes (labels)
\begin{itemize}
    \item $f(\vec{x})$ returns the correct class for $\vec{x}$
    \item 2 classes: $f(\vec{x})$ is a Boolean function: binary classification, concept learning (T/F or 0/1 or -1/+1 or neg/pos)
    \item $>$ 2: multi-class problem $(C_1,C_2,C_3,...,C_K)$
\end{itemize}
It can be also viewed as the allocation of the input space in decision regions 0/1 (e.g. linear separators)
\subsection{Regression}
Process of estimating of a real-valued function on the basis of finite set of noisy samples.

\chapter{Models}
The model defines the class of functions that the learning machine can implement: it's needed to capture/describe relationships among the data (on basis of the task) by a "language". This language is related to the representation used to get knowledge.
\begin{itemize}
    \item Linear models: representation of H defines a continuously parametrized space of potential hypothesis, each assignment of $w$ is a different one, e.g.:
    $$\text{Binary classifier:  }h(\vec{x})=sign(\vec{w}^T\vec{x}+w_0)$$
    $$\text{Linear regression:  }h_w(\vec{x})=\vec{w}^T\vec{x}$$
    \item Symbolic rules: hypothesis space is based on discrete representations, different rules are possible, eg.
    $$\text{Binary classifier:  }\text{if ($x_1=0$) and ($x_2=1$) then $h(\vec{x})=1$ else $h(\vec{x})=0$}$$
    \item Probabilistic models: estimate $p(x,y)$
    \item K-nearest neighbor regression: predict mean $y$ value of nearest neighbors (memory-based)
\end{itemize}

\chapter{Learning Algorithm}
Basing on data, task and model, search through the hypothesis space $H$ of the best hypothesis.
$H$ may not concide with the set of all possible functions and the search can not be exhaustive: we need to make assumptions $\rightarrow$ inductive bias.
\section{The role of the inductive bias}
In order to set up a model and a learning algorithm we can make assumptions (about the nature of the target function) concerning either
\begin{itemize}
    \item constraints in the model (in the hypothesis space $H$, due to the set of hypothesis that we can express or consider) (LANGUAGE BIAS)
    \item constraints or preferences in learning algorithms/search strategy (SEARCH BIAS)
    \item both
\end{itemize}
Such assumptions are strictly needed to obtain a useful model for the ML aims (model with generalization capabilities)
\section{Find the Version Space (VS)}
We call the version space $VS_{H,TR}$ with respect to hypothesis space H and training set TR, the subset of hypothesis from H consistent with all training examples.
\begin{itemize}
    \item h is consistent with the TR if $h(\vec{x})=d(\vec{x})$ for each training example $<\vec{x}, d(\vec{x})>$ in TR.
\end{itemize}
\section{Unbiased Learner}
An unbiased learner is unable to generalize. A learner that makes no prior assumptions regarding the identity of the target function/concept has no rational basis for classifying any unseen instances.
Bias not only is assumed for efficiency, it is also needed for generalization capability. However, it does not tell us which one is the best solution for generalization yet.

\section{Tasks and Loss}
Using a "inner" loss function/measure to judge the "goodness" of approximation. High value of loss means poor approximation.
$$L(h_{\vec{w}}(\vec{x}),d)$$
The Error (or Risk or Loss) is an expected value of this $L$ over the set of samples:
$$Loss(h_{\vec{w}})=E(\vec{w})=\frac{1}{l}\sum_{p=1}^lL(h_{\vec{w}}(\vec{x}_p),d_p)$$
$L$ can change for different tasks.
\subsection{Regression}
Predicting a numerical value.
\begin{itemize}
    \item Output: $d_p=f(\vec{x}_p)+e$ (real value function + random error);
    \item $H$: a set of real-valued functions;
    \item Loss function $L$: the squared error $$L(h_{\vec{w}}(\vec{x}_p), d_p)=(d_p-h_{\vec{w}}(\vec{x}_p))^2$$
    \item The mean over the dataset provide the MSE mean square error.
\end{itemize}
\subsection{Classification}
Classification of data into discrete classes.
\begin{itemize}
    \item Output: e.g. ${0,1}$;
    \item $H$: a set of indicator functions;
    \item Loss function $L$: the 0/1 Loss $$L(h_{\vec{w}}(\vec{x}_p), d_p)=\begin{cases}0 \ \ if &h_{\vec{w}}(\vec{x}_p)=d_p \\ 1 &otherwise \end{cases}$$
    \item The mean over the dataset provide the number/percentage of misclassified patterns and so the accuracy.
\end{itemize}

\section{Generalization}
Generalization error measures how accurately the model predicts over novel samples data. Error/loss computed over new data.
\begin{itemize}
    \item Learning phase (training, fitting): build the model from known data - TR and bias.
    \item Predictive or test phase: apply the model to new examples, making evaluation of the generalization capability of our predictive hypothesis.
    \item Theory: e.g. statistical learning theory (SLT, Vapnik) answers under what mathematical conditions a model is able to generalize. 
\end{itemize}
\section{Validation}
Evaluation of performances for ML systems = generalization/predictive accuracy evaluation.
Validation techniques to:
\begin{itemize}
    \item evaluate (model assessment).
    \item manage the generalization capability (model selection).
\end{itemize}
\section{Generalization issues}
Overfitting: a learner overfits the data if it outpus a hypothesis $h(\cdot)\in H$ having true/generalization error (risk) R and empirical (training) error E, but there is another $h'(\cdot)\in H$ having $E'>E$ and $R'<R$ so that $h'$ is a better choice, despite a worst fitting.

Critical aspect: accuracy/performance estimation both theoretical and empirical.
\chapter{Complexity control}
\section{Complexity on a case of study}
In an example of parametric model for regression the complexity of the hypothesis increases with the degree $M$ of the polynomials of the set of functions.

$l=$ number of examples (patterns).
$$h_{\vec{w}}(x)=w_0+w_1x+w_2x^2+\dots+w_Mx^M = \sum_{j=0}^Mw_jx^j$$
$$E(\vec{w})=\sum_{p=1}^l(y_p-h_{\vec{w}}(x_p))^2$$
Minimize $E(\vec{w})$ and find this way the best $\vec{w}*$
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{overfitting.png}
\end{figure}
\section{Formal setting}
Approximate unkown $f(\vec{x})$, $d$ is the target = true f + noise.
Given value of $d$, the probability distribution $P(\vec{x}, d)$ and a loss or cost function e.g. $$L(h(\vec{x}, d)) = (d-h(\vec{x}))^2$$
We ask to search in $H$ to minimize risk function $$R = \int L(d, h(\vec{x}))dP(\vec{x}, d)$$
But we have only the finite data set TR $= (\vec{x}_p, d_p)$ $p=1,\dots,l$.
To seaerch h: minimize empirical risk (training error $E$), finding the best values for the model free parameters
$$R_{emp}(h, TR)=\frac{1}{l}\sum_{p=1}^l(d_p-h(\vec{x}_p))^2$$
\section{Vapnik-Chervonenkis-dim and Statistical Learning Theory (SLT)}
A general theory for the problem concerning all these topics.
Given a measure of complexity of the $H$ space (VC-dim, shortly VC) we ask if we can use $R_{emp}$ to approximate $R$.
\begin{itemize}
    \item It holds with probability $1-\delta$ that the guaranteed risk:
    $$R \leq R_{emp} + \epsilon\left(1/l, VC, 1/\delta\right)$$
    where $\epsilon$ is the VC-confidence.
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{vapnik.png}
\end{figure}

\chapter{Validation}
After models training on the TR set $\rightarrow$ evaluation of the performances.

Generalization/predictive accuracy evaluation.
\section{Two aims}
\begin{itemize}
    \item Model selection: estimating the performance of different learning models in order to choose the best one to generalize (generalization error). This includes searching the best hyperparameteres of the model (e.g.polynomial order,...). It returns a model;
    \item Model assessment: having chosen a final model, estimating/evaluating its prediction error/risk on new test data. It returns an estimation.
\end{itemize}
\textbf{Golden rule:} keep separation between goals and use separate datasets.
\section{Validation: ideal world}
\begin{itemize}
    \item A large TR to find the best hypothesis;
    \item a large VS validaition set for model selection;
    \item a very large external unseen data TS test set.
\end{itemize}
Basic techniques to estimate the generalization performance:
\begin{itemize}
    \item Simple hold-out (basic setting)
    \item K-fold Cross Validation
\end{itemize}

\section{Hold out cross validation}
Hold out: basic setting. Partition dataset D into TR, VL and TS disjoint sets.
\begin{itemize}
    \item TR used to training the algorithm;
    \item VL can be used to select the best model (e.g. hyperparams tuning);
    \item TS (result) is not to be used for any tuning/selection but it's only for model assessment.
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{schema.png}
\end{figure}
\section{K-fold cross validation}
Hold out can make insufficient use of data.
K-fold cross validation consists in:
\begin{itemize}
    \item splitting dataset D into K mutually exclusive subsets $D_1,\dots,D_K$;
    \item training the learning algorithm on $D\setminus D_i$ and test it on $D_i$;
    \item Can be applied for both VL or TS splitting.
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics{kfold.png}
\end{figure}

\section{Classification accuracy}
\subsection{Confusion matrix}
Actual vs Predicted results.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{confusion.png}
\end{figure}
We can define:
\begin{itemize}
    \item Specifity (true negative rate) $$Sp = \frac{TN}{FP+TN}$$
    \item Sensitivity (true positive rate) $$Sens = \frac{TP}{TP+FN}$$
    \item Precision $$Pr = \frac{TP}{TP+FP}$$
    \item Accuracy $$Acc = \frac{TP+TN}{Tot}$$
\end{itemize}

\chapter{Linear Models}
\section{Regression}
$$h_{\vec{w}}(\vec{x})=out=w_1x+w_0$$
$$(\vec{x}_p,y_p) \ \ p=1,\dots,l$$
LMS least mean square of errors:
$$Loss(h_{\vec{w}})=E(\vec{w})=\sum_{p=1}^l(y_p-h_{\vec{w}}(x_p))^2=\sum_{p=1}^l(y_p-w_1x_p-w_0)^2$$
Note: to obtain the mean divide by $l$
\subsection{Solve it}
$$\frac{\partial E(\vec{w})}{\partial w_i}=0$$
Same solutions for $w_0$ and $w_1$ of the LMS problem.
\subsection{Notation for multidimensional inputs}
$$\vec{w}^T\vec{x}+w_0=w_0+w_1x_1+\dots+w_nx_n=w_0+\sum_{i=1}^nw_ix_i$$
$w_0$: intercept, threshold, bias, offset.......
If we put $x_0=1$ we can state an inner product:
$$\begin{cases}
\vec{x}^T=\left[1,x_1,\dots,x_n\right]\\ \vec{w}^T=\left[w_0,w_1,\dots,w_n\right]
\end{cases} \implies \vec{w}^T\vec{x}=\vec{x}^T\vec{w}$$
$$h(\vec{x}_p)=\vec{x}_p^T\vec{w}$$
\section{Classification}
We reuse the linear model. In this case categorical targets e.g. 0/1 or -1/+1.

$$\vec{w}^T\vec{x}+w_0 = 0$$ 
It defines a hyperplane through which we can descern among negative or positive values and classificate.
$$h(\vec{x})=\begin{cases}
1 \ \ &if \ \ \vec{w}^T\vec{x} + w_0 \geq 0 \\
0 \ \ &otherwise 
\end{cases} \iff [0,1] \ \ \text{output range}$$

$$h(\vec{x})=sign(\vec{w}^T\vec{x}+w_0) \iff [-1,+1] \ \ \text{output range}$$
The classification may be viewed as the allocation of the input space in decision regions (Linear threshold unit LTU in the form $\vec{w}^T\vec{x}=-w_0$).

\section{Learning algorithms}
Two algorithms both based on LMS:
\begin{itemize}
    \item Normal equation solution (direct approach);
    \item Gradient descent (iterative approach).
\end{itemize}
\subsection{Normal equation algorithm}
Finding the $\vec{w}*$ that minimizes the expected loss on TR data
$$R_{emp} = \frac{1}{l}\sum_{p=1}^lL(h(\vec{x}_p),y_p) = \frac{1}{l}\sum_{p=1}^l(y_p-\vec{w}^T\vec{x})^2$$
\begin{itemize}
    \item $$\frac{\partial E(\vec{w})}{\partial w_j}=2\sum_{p=1}^l(y_p-\vec{x}_p^T\vec{w})x_{p,j}$$
    \item We can get the normal  (imponing derivative $=0$): $$\left(X^TX\right)\vec{w}=X^T\vec{y}\implies \vec{w}=\left(X^TX\right)^{-1}X^T\vec{y}= X^+\vec{y}$$
    Where "+" denotes the Moore-Penrose pseudoinverse of the matrix (it always exists)
    \item The solution are infinite: we can choose the min norm$(\vec{w})$ solution
\end{itemize}
\subsubsection{Computing the pseudoinverse of X}
We write X in its singular value decomposition (SVD) so computing the pseudoinverse is easy.
$$X = U\Sigma V^T \implies X^+ = V \Sigma^+U^T$$
Where $\Sigma$ is diagonal and so its pseudoinverse is obtained by replacing every nonzero entry by its reciprocal

\subsection{Gradient descent}
The gradient shows the direction where the function grows more.
Its negative shows the direction where the function decreases and becomes flat.
\begin{itemize}
    \item $$\frac{\partial E(\vec{w})}{\partial w_j} = -2\sum_{p=1}^l(y_p-\vec{x}_p^T\vec{w})\vec{x}_{p,j}$$
    \item Delta rule: $$\vec{w}_{new}=\vec{w}+\eta\Delta\vec{w}$$
    It's an error correction rule, where the "error" of the weights is: $$\Delta \vec{w} = -\vec{\nabla}_{\vec{w}}E(\vec{w})$$
    and $\eta$ is the learning rate parameter (step size) which rules the speed of the gradient descending.
\end{itemize}
\subsubsection{Batch/On-line version of gradient descent}
\begin{itemize}
    \item For batch version the gradient is the sum over all the $l$ patterns:
$$\frac{\partial E(\vec{w})}{\partial w_j}=-2\sum_{p=1}^l(y_p-\vec{x}_p^T\vec{w})x_{p,j}$$
And the weights are updated all together after evaluating this sum (after one "epoch")
    \item For the on-line/stochastic version we update weights pattern after pattern, so the error depents on it:
$$\frac{\partial E_p(\vec{w})}{\partial w_j} = -2(y_p-\vec{x}_p^T\vec{w})x_{p,j} = -\Delta_pw_j$$ 
It could be faster, but it needs smaller learning rate.
\end{itemize}
There exist also intermediate cases like mini-batch training.
\section{Linear models: inductive bias}
\begin{itemize}
    \item Language bias (constraints of the model): the H is a set of linear functions (may be very restrictive and rigid)
    \item Search bias (preferences on the learning algorithm): ordered search guided by the Least Squares Minimization goal
\end{itemize}
\section{Extending linear model: LBE}
Basis transformations: linear basis expansion (LBE):
$$h_{\vec{w}}(\vec{x})=\sum_{k=0}^Kw_k\phi_k(\vec{x})$$
$$\phi_k \ \ : \ \ \mathcal{R}^n \rightarrow \mathcal{R}$$
The expansion is linear but the basis are transformations of $\vec{x}$ which can be non linear (log, polynomials, roots....)
So the model is linear in the parameters and also in $\phi$, but not in $\vec{x}$: we can use same learning algorithms as before.
It can be applied to both regression and classification.
\begin{itemize}
    \item PROS: can model more complicated relationships, it is more expressive;
    \item CONS: with large basis of functions, we easily risk overfitting, hence we require methods for controlling the complexity of the model.
    Whereas complexity is not referring at any computational cost, but it's just a measure of the flexibility of the model to fit the data (VC-dim)
\end{itemize}
\section{Improvements: Tikhonov regularization}
Smoothed model is possible, just add contraints to the sum of value of $|w_j|$ penalizing models with high values of $|\vec{w}|$
$$Loss(\vec{w})=\sum_{p=1}^l(y_p-\vec{x}_p^T\vec{w})^2+\lambda||\vec{w}||^2$$
i.e. we are favoring this way "sparse" models using less terms due to weights $w_j=0$ or close to 0 so basically a less complex model.
$\lambda$ is the regularization (hyper)parameter.
\subsection{Solving it}
\begin{itemize}
    \item Direct approach:
    $$\vec{w}=\left(X^TX+\lambda I\right)^{-1}X^T\vec{y}$$
    \item Gradient approach $\implies$ weight decay technique:
    $$\vec{w}_{new}=\vec{w}+\eta\Delta\vec{w}-2\lambda\vec{w}$$
\end{itemize}
\subsection{Tikhonov regularization: trade-off}
$$Loss(\vec{w})=\sum_{p=1}^l\left(y_p-\vec{x}_p^T\vec{w}\right)^2+\lambda||\vec{w}||^2$$
The trade-off is of course ruled by the value of $\lambda$:
\begin{itemize}
    \item small $\lambda$ leads to the risk of overfitting;
    \item high $\lambda$ leads to the risk of underfitting.
\end{itemize}

\chapter{KNN}
K-nearest neighbor. Still simple but flexible and local.
\section{1-nn}
\begin{itemize}
    \item Simply store the training data $$<\vec{x}_p,y_p> \ \ p=1,\dots,l$$
    \item Given an input $\vec{x}$ of dimension $n$
    \item Find the nearest training example $\vec{x}_i$ (i-th pattern) s.t.:
    $$\vec{x}_i = \arg \min_pd(\vec{x},\vec{x}_i)$$
    Where $d$ measures a distance e.g. Euclidian distance:
    $$d(\vec{x},\vec{x_p})=\sqrt{\sum_{t=1}^n\left(x_t-x_{p,t}\right)^2}=||\vec{x}-\vec{x}_p||$$
    \item Then output $y_i$
\end{itemize}

\section{K-nn}
A natural way to classify a new point is to have a look at its neighbors and take an average.
Denoting $N_k(\vec{x})$ the neighborhood of $\vec{x}$ that contains exactly $k$ neighbors (closest patterns according to distance $d$), we have:
$$avg_k(\vec{x})=\frac{1}{k}\sum_{\vec{x}_i\in N_k(\vec{x})}y_i$$

Majority Voting: if there is a clear dominance of one of the classes in the neighborhood of the observation then it is likely that the observation itself would belong to that class aswell. So, for targets in $[0,1]$:
$$h(\vec{x})=\begin{cases}
    1 &if \ \ avg_k(\vec{x})>0.5\\ 0 &otherwise
\end{cases}$$

While for regression tasks we can use directly the average mean over K-nn.

\section{Limits of K-nn}
\begin{itemize}
    \item The computational cost is deferred to the prediction phase
    \item Computationally intensive in time and space (all the training data)
    \item Offer little interpretation (subjectivity of interpretation, dependancies on the metrics)
    \item Curse of dimensionality: when there are lots of input variables it totally fails
    \item Always in high dim, there are some problems: hard to find nearby points; low sampling density; irrelevant features issues.
\end{itemize}

\chapter{Bias/Variance and ensembling}
A training set is only one possible realization from the universe of data: different TR sets can provide different estimates

Decomposition of the expected error at a point $\vec{x}$:
\begin{itemize}
    \item Bias: quantifies the discrepancy between true function and $h(\vec{x})$: the smaller is H, the higher is the bias
    \item Variance: quantifies the variability of the response of model $h$ for different realizations of the training data: the higher is the flexibility, the higher the variance
    \item Noise: because the labels include random error (e.g. for a given $\vec{x}$ there are more than one possible $d$)
\end{itemize}
\section{Bias-Var. Analysis}
Assuming that the data points are drawn i.i.d. from a unique probability distribution $P$
The goal of the analysis is to compute, for an arbitrary new point $\vec{x}$, $$E_P\left[\left(y-h(x)\right)^2\right]$$
Where $y$ is the value for $\vec{x}$ that could be present in a dataset, and the expectation is over all training sets drawn according to $P$. We will decompose this expectation into three components: bias, variance and noise.


We now that $$\begin{cases}Var[Z]=E_P[(Z-\bar{Z})^2]$$ \\
$$Var[Z] = E[Z^2]-\bar{Z}^2 \implies E[Z^2]=\bar{Z}^2+Var[Z]\end{cases}$$

$$E_P\left[\left(y-h(x)\right)^2\right] = E_P[h(x)^2-2yh(x)+y^2]=E_P[h(x)^2]+E_P[y^2]-2E_P[y]E_P[h(x)]$$
So using the variance lemma above $$E_P[h(x)^2]=\bar{h}(x)^2+E_P[(h(x)-\bar{h}(x))^2]$$
And for the second term $$E_P[y^2]=f(x)^2+E_P[(y-f(x))^2]$$
Note that we used $E_P[y] = E_P[f(x)+\epsilon] = f(x)$ cause the noise is gaussian with 0 mean.

Putting all together and doing some calculations we find:
$$E_P[(y-h(x))^2]=E_P[(h(x)-\bar{h}(x))^2]+(\bar{h}(x)-f(x))^2+E_P[(y-f(x))^2]$$
And we can clearly see that first term is the variance of our model, the second is the $(\text{bias})^2$ and the third is the $(\text{noise})^2$.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{biasvar.png}
\end{figure}

\section{Bias/Var and regularization}
Recall regularization:
$$Loss(\vec{w})=\sum_p(d-p-o(\vec{x}_p))^2+\lambda||\vec{w}||^2$$
Varying $\lambda$ we can obtain more complex solutions (low $\lambda$) or more regularized and simple solutions (high $\lambda$),
and of course there are consequences on bias and variance of our model.
\begin{itemize}
    \item High $\lambda$: higher bias, lower variance
    \item Decreasing $\lambda$: we are more dependent on the specific training data
    \item Very low $\lambda$: low bias, high variance; on average the best situation.
\end{itemize}
\section{Ensemble Learning}
Take advantage of multiple models: simplest case "voting schema".
\begin{itemize}
    \item Regression:
    Average the output: $$\bar{h}(\vec{x})=\frac{1}{K}\sum_{i=1}^Kh_i(\vec{x})$$
    \item Classification:
    Take a vote over many classifier - we can proceed with some methods like bagging, boosting.
\end{itemize}
\subsection{Bagging (bootstrap aggregating)}
Bagging: train $K$ classifiers on different subsets of training set and differentiate each training using bootstrap (resampling with replacement)
The average of $h$ reduces the variance.
\subsection{Boosting (e.g. AdaBoost)}
If models have the same errors there is not such advantage on ensembling.
Boosting can solve this issue greatly improving performance of weak learners. It consists in:
\begin{enumerate}
    \item Differentiate each training focusing on errors (more weight to difficult instances)
    \item Combine results by output weights (weighted vote for classifiers, with more weight to low error classifiers)
\end{enumerate}
But it can suffer for noisy data (those are accidentally weighted more)

\chapter{Networks models}
\section{Artificial neuron: processing unit}
\begin{itemize}
    \item Node, neuron or unit;
    \item Input: from extern source or other units
    \item Input connections: weights $\vec{w}$ free parameters; modified by learning.
\end{itemize}
$$\begin{cases}
    net_i(\vec{x})=\sum_jw_{ij}x_j\\
    o_i(\vec{x})=f(net_i(\vec{x}))
\end{cases}$$
The weighted sum $net_i$ is called the net input to unit $i$ and the function $f$ is the unit's activation function (e.g.linear,LTU,...)
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{unit.png}
\end{figure}
\subsection{Perceptron}
Single neuron with LTU as activation function. Linear treshold unit. 
\subsection{McCulloch and Pitts networks}
Neurons in two possible states: firing (1) and not firing (0). All synapses are equivalent and characterized by a single real number (their strenght, $\vec{w}$) which is positive for excitatory connections and negative for inhibitory ones.
\begin{itemize}
    \item a neuron $i$ becomes active when the sum of those connections $w_{ij}$ coming from neurons $j$ connected to it which are active, plus a bias, is larger than zero. Binary inputs $\rightarrow$ binary output.
    \item "basically" the LTU/perceptron
\end{itemize}
\subsection{Properties of Networks of perceptrons}
\begin{itemize}
    \item Perceptrons can represent any universal boolean functions like AND, OR, NOT (NAND, NOR)
    \item Only two levels deep is sufficient (more can be more efficient)
    \item Single layer cannot model all the possible functions due to the linear seaparable problems
    \item No provided (up to now) learning algorithm even for one unit!
\end{itemize}
\subsection{Learning for one unit model - Overview}
Two kinds of method for learning (also historical view)
\begin{itemize}
    \item Adaline (Widrow, Hoff, 1959) = Adaptive Linear Neuron: linear unit during training; LMS direct solution and gradient descent solution
    \item Perceptron (Rosenblatt, 1957) = non-linear unit during training: with hard limiter or threshold activation function (ONLY CLASSIFICATION)
\end{itemize}
\subsection{The perceptron learning algorithm}
Rosenblatt (1958-1962). Based on minimizing the number of misclassified patterns:
$$\text{Find $\vec{w}$ s.t.} sign(\vec{w}^T\vec{x})=d$$
It's an on-line algorithm: a step can be made for each input pattern.
\begin{enumerate}
    \item Initialize the weights (either to zero or to a small random value)
    \item pick a learning rate $\eta$ (this is a number between 0 and 1)
    \item until stopping condition is satisfied (e.g. weights don't change): for each training pattern $(\vec{x}, d)$ with d=+1/-1 let $out$ be:
    $$out = sign(\vec{w}^t\vec{x}) \in \left[-1,+1\right]$$
    \item if $out=d$ don't change weights (i.e. "minimize only misclassifications"!)
    \item if $out\neq d$ update the weights:
    $$\vec{w}_{new} = \vec{w}+\eta d \vec{x} \ \ \text{add} \ \begin{cases}
        + \eta \vec{x} \ \text{if $\vec{w}^T\vec{x}\leq 0$ and $d=+1$}\\
        - \eta \vec{x} \ \text{if $\vec{w}^T\vec{x}>0$ and $d=-1$}
    \end{cases}$$
    Or in a unique different form:
    $$\vec{w}_{new} = \vec{w} + \frac{\eta}{2}\left(d-out\right)\vec{x}$$
\end{enumerate}
\subsection{Perceptron's Convergence Theorem}
The perceptron with the perceptron learning algorithm is always able to learn what it is able to represent: linear decision boundaries.
\begin{center}
"The perceptron is guaranteed to converge (classifying correctly all the input patterns) in a finite number of steps if the problem is linearly separable".
\end{center}
This independently from the starting point, even if the final solution could depend on it. May be "unstable" if the problem is not separable.
\subsection{Perceptron Learn. Alg. Vs LMS alg.}
\begin{itemize}
    \item Min. misclassifications $out = sign(\vec{w})^T\vec{x}$ / Vs / Min. $E(\vec{w})$ with $out=\vec{w}^T\vec{x}$;
    \item Always converges for linear separable problems to a perfect classifier / Vs / Asymptotic convergence also for not linear separable problems;
    \item Difficult to be extended to networks of units (NN) / Vs / It can be extended to networks of units (NN), using the gradient based approach.
\end{itemize}

\section{Activation functions}
These functions are used to "activate" units. They can be linear as the case of perceptron/LTU,or non-linear as squashing functions like sigmoidal logistic function. Here some examples follow.
\begin{itemize}
    \item Sigmoidal (logistic) function
    \begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{sigmoid.png}
    \end{figure}
    \item Hyperbolic tangent
    \begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{tanh.png}
    \end{figure}
    \item Radial Basis Function (RBF, gaussian)
    \begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{gaussian.png}
    \end{figure}
    \item Rectified Linear Unit (ReLu)
    \begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{relu.png}
    \end{figure}    
    \item Softplus (smooth approximation of ReLu): $$f(x)=ln(1+e^x)$$
\end{itemize}
All these functions provide continuous outputs but it is always possible to assign e.g. for logistic function positive classes to all the values $\geq 0.5$ and negative classes to all the others. It is always possible to change the threshold and even to consider a rejection zone in an interval around the threshold to avoid fragile decisions.
\subsection{Activation functions: derivatives}
These activation functions are useful to us also because their derivatives have an easy analytical form and computing them is so simple.
(NOTE: except done for the step function because it's not continuous!)

Some examples, for sigmoid and tanh, for any values of $a$:
$$\frac{df_\sigma(x)}{dx}=f_\sigma(x)(1-f_\sigma(x))$$
$$\frac{df_{tanh}(x)}{dx}=1-f_{tanh}(x)^2$$

\section{Neural networks (NN)}
NN models tradionally presented by the type of:
\begin{itemize}
    \item UNIT: net, activation functions;
    \item ARCHITECTURE: number of units, topology, number of layers;
    \item LEARNING ALGORITHM.
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{feedforwardnn.png}
\end{figure}    
The architecture of a NN defines the topology of the connections among the units.
\subsection{The Multi Layer Perceptron (MLP)}
A multilayer perceptron (MLP) is a name for a modern feedforward neural network consisting of fully
connected neurons with nonlinear activation functions, organized in layers, notable for being able
to distinguish data that is not linearly separable.
\subsection{Feedforward processing}
Input $\longrightarrow$ output:

For each input pattern $\vec{x}$:
\begin{enumerate}
    \item The input pattern is loaded in the input layer;
    \item We compute the output of all the units of the 1st hidden layer;
    \item We compute the output of all the units of the 2st hidden layer and so on for all the hidden layers;
    \item We compute the output of all the units of the output layer (NN output $h(\vec{x})$);
    \item We can now compute the error (delta) at the output level.
\end{enumerate}

\subsection{NN flexibility: which tasks?}
H space: continuous space of all the functions that can be represented by assigning the weight values of the given architecture.
Note that, depending on the class of values produced by the network output units, discrete or continuous, the model can deal respectively with classification or regression tasks.
$$h(\vec{x})=f_k\left(\sum_jw_{kj}f_j\left(\sum_iw_{ji}x_i\right)\right)$$
Also, if using multiple output units, we can deal with multi-regression or multi-classes classifications.
\subsection{NN as a function}
$$h(\vec{x})=f_k\left(\sum_jw_{kj}f_j\left(\sum_iw_{ji}x_i\right)\right)$$
\begin{itemize}
    \item This is the function computed by a two-layer feedforward neural network
    \item Units and architecture are just a graphical representation of the data flow processing
    \item Each $f_j\left(\sum_iw_{ji}x_i\right)$ can be seen as computed by an independent processing element (unit, hidden unit) or a special kind of $\phi$ of LBE
    \item also, NN is a function non linear in the parameters $\vec{w}$
\end{itemize}
\subsubsection{NN compared to LBE}
\begin{itemize}
    \item LBE (recall): fixed linear basis functions
    $$\text{Regression:}\ \ h(\vec{x})=\sum_jw_j\phi_j(\vec{x}) \ \ \ \ \ \text{Classification:}\ \ h(\vec{x})=f\left(\sum_jw_j\phi_j(\vec{x})\right)$$
    \item Neural Network:
    $$\begin{cases}
        h(\vec{x})=f_k\left(\sum_jw_{kj}f_j\left(\sum_iw_{ji}x_i\right)\right) = f\left(\sum_jw_j\phi_k(\vec{x},\vec{w})\right)\\
        \phi_j(\vec{x},\vec{w})=f_j\left(\sum_iw_{ji}x_i\right)
    \end{cases}$$
\end{itemize}
So we can see that basically NN are adaptive and flexible types of LBE: now the $\phi$ depends on weights so they can change with data training.
The basis functions themselves are adapted to data by fitting the $\vec{w}$.

So $h(\vec{x})$ is just a nonlinear function of weighted sums of nonlinearly transformed linear models + the important enhancement of adaptivity.
\subsection{Universal approximation}
The flexibility of NNs is theoretically grounded.
\begin{center}
"A single hidden-layer network with logistic activation functions can approximate arbitrarily well every
continuous (on hypercubes) function provided enough units in the hidden layer".
\end{center}
This is an existence theorem. This not provide the algorithm nor the number of units required.

\section{The learning algorithms: back-propagation}
The basic architecture is a feed-forward fully connected neural network (MLP). We use different
indices, $k$, $j$ and $i$, as generic indices for the different layers of the MLP.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{backprop.png}
\end{figure}    
\begin{itemize}
    \item The problem: to estimate the contribution of hidden units to the error at the ouput level. We do this by computing the Generalized Delta Rule.
    \item Training set (TR): $\left\{\left(\vec{x}^1,d^1\right),\dots,\left(\vec{x}^l,d^l\right)\right\} \rightarrow$ supervised learning.
    Each input unit $i$ is loaded with the input $x_i$ (component $i$ of the input vector $\vec{x}$), hence $o_i=x_i$ per each pattern $p=1,\dots,l$.
    \item Total Error: 
    $$E_{tot}=\sum_pE_p \ \ \ E_P = \frac{1}{2}\sum_{k=1}^K\left(d_k-o_k\right)^2$$
    $E_p$ is computed for the input pattern $p$ for all the $K$ units.
    \item Objective: find the values of parameters $\vec{w}$ that minimize $E_{tot}$
    \item Least Mean Square (LMS): the minimization of the above total error.
    \item Idea: perform a descent on the surface of $E_{tot}$ on the basis of the gradient.
\end{itemize}
\subsection{Iterative gradient descent training algorithm}
Back-propagation (the same presented in general for the LMS)

$$\text{Compute units outputs and $E_{tot}$}$$;
$$\text{While $E_{tot} > \epsilon$ (desired value or other criteria) do:}$$
$$\begin{aligned}
    \forall w \in \vec{w}: &\Delta w = - \frac{\partial E_{tot}}{\partial w} \\ &w^{new} \leftarrow w + \eta \Delta w + ...
\end{aligned}$$
$$\text{Compute units outputs and $E_{tot}$;}$$
$$\text{end.}$$
Backpropagation focus mainly on step 1: computing the gradient.
$$\Delta \vec{w}=-\frac{\partial E_{tot}}{\partial \vec{w}}= - \sum_p\frac{\partial E_p}{\partial \vec{w}} = \sum_p \Delta_p\vec{w}$$
Whereas $p$ is the p-th pattern to be fed as input to the neural network.

Then, for $w_{tu}$ of a generic unit $t$, pattern $p$, and the input $u$ from a generic unit $u$ to the unit $t$, we have:
$$\Delta_pw_{tu}=-\frac{\partial E_p}{\partial net_t}\frac{\partial net_t}{\partial w_{tu}}=\delta_t o_u$$
$$\begin{cases}
    net_t=\sum_sw_{ts}o_s\\
    o_t=f_t(net_t)
\end{cases}\implies \frac{\partial net_t}{\partial w_{tu}}=\frac{\partial\sum_sw_{ts}o_s}{\partial w_{tu}}=o_u$$
Now we expand on $\delta_t$ (the delta of a unit $t$):
$$\delta_t=-\frac{\partial E_p}{\partial net_t}=-\frac{\partial E_p}{\partial o_t}\frac{\partial o_t}{\partial net_t}=-\frac{\partial E_p}{\partial o_t}f_t'(net_t)$$
Now we recall that $E_p=\frac{1}{2}\sum_{k=1}^K\left(d_k-o_k\right)^2$ and we have to distinguish 2 cases, depending on whether $o_t$ is an output unit $k$ (case 1) or an hidden unit $j$ (case 2).
\subsubsection{Case 1: Output Unit (t=k)}
$$-\frac{\partial E_p}{\partial o_k}=-\frac{\partial \frac{1}{2}\sum_{r=1}^K\left(d_r-o_r\right)^2}{\partial o_k}=(d_k-o_k)$$
$$\delta_k = -\frac{\partial E_p}{\partial net_k}=(d_k-o_k)f_k'(net_k)$$
\subsubsection{Case 2: Hidden unit (t=j)}
$$-\frac{\partial E_p}{\partial o_j}=\sum_{k?1}^K-\frac{\partial E_p}{\partial net_k}\frac{\partial net_k}{\partial o_j}=\sum_{k=1}^K\delta_k w_{kj}$$
$$\text{since} \frac{\partial net_k}{\partial o_j}=\frac{\partial \sum_sw_{ks}o_s}{\partial o_j}=w_{kj}$$
$$\implies\delta_j=\left(\sum_{k=1}^K\delta_kw_{kj}\right)f_j'(net_j)$$
So we can see from this latter formula that we can back-propagate the signals $\delta_k$ that we had already obtained from the output to the hidden unit $j$.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{backprop2.png}
\end{figure}  

\section{Learning rate - improvements}
Act on learning rate $\eta$ for a better efficiency:
\begin{enumerate}
    \item Nesterov momentum;
    \item Variable learning rate;
    \item Adaptive learning rates (change during training and for each $\vec{w}$);
    \item Changing it depending on the layer (higher for deep layers) or higher for few input connections.
\end{enumerate}
\subsection{Nesterov Momentum}
$$0<\alpha: \text{momentum parameter} < 1$$
$$\Delta \vec{w}_{new}=-\eta\frac{\partial E(\vec{w})}{\partial w}+\alpha\Delta\vec{w}_{old}$$
Note: here $\eta$ is included inside the correction for $\vec{w}_{new}$, differently from before.
$$\vec{w}_{new}=\vec{w}+\Delta\vec{w}_{new}$$
\chapter{Support Vector Machines (SVM)}
Notation:
\begin{itemize}
    \item $N$ is the number of examples;
    \item $m$ is the dimension of the input vector;
    \item $b$ instead of $w_0$ as the intercept or bias.
\end{itemize}
\section{SVM for binary classification}
\begin{itemize}
    \item Linear machine (LTU, Perceptron, ...)
    \item Maximization of the separation margin
    \item Structural risk minimization
\end{itemize}
Initially (hard margin SVM) we assume linear separable problems and no errors in data.
\subsection{Linear separable patterns - Hard margin SVM}
Given the training set $TR=\left\{\left(\vec{x},_i,d_i\right)\right\}_{i=1}^N$

Find an hyperplane of equation $\vec{w}^T\vec{x}+b=0$ to separate the examples:
$$\begin{aligned}
    \vec{w}^T\vec{x}_i+b\geq0 \ for d_i=+1\\
    \vec{w}^T\vec{x}_i+b<0 \ for d_i=-1
\end{aligned}$$
\begin{itemize}
    \item $g(\vec{x})=\vec{w}^T\vec{x}_i+b$ is the discriminant function;
    \item $h(\vec{x})=sign(g(\vec{x}))$ is the hypothesis.
\end{itemize}
\subsubsection{Separation margin}
The separation margin $\rho$ is evaluated as (the double of) the distance between the linear hyperplane and the closest data point. We can think to a "safe zone".
\subsubsection{Optimal hyperplane}
The optimal hyperplane is the hyperplane which maximizes the margin $\rho$:
$$\vec{w}_o^T\vec{x}+b_o=0$$
We will find: $$\rho=\frac{2}{||\vec{w}||}$$
So maximize $\rho\iff$ minimize $||\vec{w}||$
\subsubsection{Canonical representation of the hyperplane}
$$\begin{aligned}
    \vec{w}^T\vec{x}_i+b\geq0 \ for d_i=+1\\
    \vec{w}^T\vec{x}_i+b<0 \ for d_i=-1
\end{aligned}$$
Re-scaling $\vec{w}$ and $b$ so that the closest points to the separating hyperplane satisfy $$|g(\vec{x}_i)|=|\vec{w}^T\vec{x}_i+b|=1$$
We can rewrite:
$$\begin{aligned}
    \vec{w}^T\vec{x}_i+b\geq1 \ for d_i=+1\\
    \vec{w}^T\vec{x}_i+b<-1 \ for d_i=-1
\end{aligned}$$
So in a compact form: $$d_i(\vec{w}^T\vec{x}_i+b)\geq1\forall i=1,\dots,N$$
\subsubsection{Support Vector}
A support vector $\vec{x}^{(s)}$ satisfies the previous equation exactly:
$$d^{(s)}(\vec{w}^T\vec{x}^{(s)}+b)=1$$
\subsubsection{Computing the distance}
The discriminant function is $g(\vec{x})=\vec{w}^T\vec{x}+b$. Recall that $\vec{w}_o$ is a vector orthogonal to the hyperplane. Let's denote the distance between $\vec{x}$ and the optimal hyperplane with $r$:
$$\vec{x}=\vec{x}_p+r\frac{\vec{w_o}}{||\vec{w}_o||}$$
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{distance.png}
\end{figure}  
Evaluating it in $g(\vec{x})$ we obtain:
$$g(\vec{x})=g(\vec{x}_p)+r\vec{w}_o^T\frac{\vec{w}_o}{||\vec{w}_o||}$$
$$\implies g(\vec{x})=0+r\frac{||\vec{w}_o||^2}{||\vec{w}_o||}=r||\vec{w}_o||\implies r=\frac{g(\vec{x})}{||\vec{w}_o||}$$
So for a positive support vector $\vec{x}^{(s)}$:
$$\vec{x}^{(s)}=\frac{g(\vec{x}^{(s)})}{||\vec{w}_o||}=\frac{1}{||\vec{w}_o||}=\frac{\rho}{2}$$
$$\implies \rho = \frac{2}{||\vec{w}_o||}$$
So the optimal hyperplane maximizes $\rho$ and minimizes $||\vec{w}||$
\subsubsection{Quadratic optimization problem}
Finding the optimum values for $\vec{w}$ and $b$ in order to maximize the margin yields to a quadratic optimization problem $\rightarrow$ Hard margin SVM
\subsubsection{Optimization problem - Primal form}
Given the training samples $T=\left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N$, find the optimum values of $\vec{w}$ and $b$ which minimize:
$$\Psi(\vec{w})=\frac{1}{2}\vec{w}^T\vec{w} \ \ \ \ \left(min(||\vec{w}||)\right)$$
satisfying the constraints (zero classification errors)
$$d_i\left(\vec{w}^T\vec{x}_i+b\right)\geq 1 \forall i=1,\dots,N$$
\begin{itemize}
    \item The objective function $\Psi(\vec{w})$ is quadratic and convex in $\vec{w}$;
    \item The constraints are linear in $\vec{w}$;
    \item Solving this problem scales with the size of the input space $m$.
\end{itemize}
\subsubsection{Solving the quadratic problem}
To solve this problem we use the Lagrangian multipliers method:
$$J(\vec{w},b,\alpha)=\frac{1}{2}\vec{w}^T\vec{w}-\sum_{i=1}^N\alpha_i(d_i(\vec{w}^T\vec{x}_i+b)-1)$$
\begin{itemize}
    \item Minimize $J$ with respect to $\vec{w} \implies \frac{\partial f}{\partial \vec{w}}=0\implies \vec{w}_o=\sum_{i=1}^N\alpha_id_i\vec{x}_i$;
    \item Minimie $J$ with respect to $b \implies \frac{\partial f}{\partial b}=0\implies \sum_{i=1}^N\alpha_id_i=0$;
    \item And we may substitute these in $J$ to study the dual form and obtain the Lagrangian multipliers $\left\{\alpha_i\right\}_{i=1}^N$
\end{itemize}
\subsubsection{Kuhn-Tucker conditions}
From the KT conditions it follows that in the saddle point of $J$:
$$\alpha_i(d_i(\vec{w}^T\vec{x}_i+b)-1)=0\forall i=1,\dots,N$$
\begin{itemize}
    \item if $\alpha_i>0$, then the $d_i(\vec{w}^T\vec{x}_i+b)=1$ and $\vec{x}_i$ is a support vector;
    \item if $\vec{x}_i$ is not a support vector then $\alpha_i=0$
\end{itemize}
Hence we can restrict the computation to $N_s$:
$$\vec{w}_o=\sum_{i=1}^{N_s}\alpha_{o,i}d_i\vec{x}_i$$
So the hyperplane depends only from support vectors.

\subsubsection{Optimization problem - Dual form}
Given the training samples $T=\left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N$, find the optimum values of the Lagrangian multipliers 
$\left\{\alpha_i\right\}_{i=1}^N$ which maximize:
$$Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_j\vec{x}_i^T\vec{x}_j$$
satisfying the constraints
\begin{itemize}
    \item $\alpha_i \geq 0\forall i=1,\dots, N$;
    \item $\sum_{i=1}^N\alpha_id_i=0$
\end{itemize}
\subsubsection{Solving dual problem}
\begin{itemize}
    \item Solve the dual problem obtaining $\left\{\alpha_{o,i}\right\}_{i=1}^N$
    \item Compute $\vec{w}_o=\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i$
    \item Compute $b_o=1-\vec{w}_o^T\vec{x}^{(s)}$ corresponding to a positive support vector $\vec{x}^{(s)}$:
    $$b_o=1-\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i^T\vec{x}^{(s)}$$
    \item IT'S NOT NEEDED to explicitly know $\vec{w}_o$
    \item It's needed to know Lagrangian multipliers (by solving the dual problem) and then compute the optimal bias $b_o$
\end{itemize}
Remember: $$\vec{w}_o=\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i$$
And the decision surface is given by:
$$\vec{w}_o^T\vec{x}+b_o=0\iff\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i^T\vec{x}+b_o=0$$
So, given the input pattern $\vec{x}$:
\begin{enumerate}
    \item Compute $g(\vec{x})=\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i^T\vec{x}+b_0$;
    \item Classify $\vec{x}$ as the sign of $g(\vec{x})$.
\end{enumerate}
Note that it is not necessary to compute $\vec{w}_o$ and the sum over $N$ can be restricted to the support vectors $N_s$
\subsection{Soft margin SVM}
In the soft margin SVM paradigm, at least one  point violate the exact-fit condition:
$$d_i(\vec{w}^T\vec{x}_i+b)\geq1$$
Admitting points inside the margin allows us to have a larger margin.

So we introduce the non-negative scalar variables (slack variables):
$$\xi_i\geq0\forall i=1,\dots,N$$
$$d_i(\vec{w}^T\vec{x}_i+b)\geq1-\xi_i\forall i=1,\dots,N$$
\subsubsection{Support vector (soft margin)}
A support vector $\vec{x}_i$ satisfies the previous exactly: 
$$d_i(\vec{w}^T\vec{x}_i+b)=1-\xi_i$$
\subsubsection{Primal problem - soft margin}
Given the training set $T = \left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N$ find the values of $\vec{w}$ and $b$ which minimize the objective function:
$$\Psi(\vec{w},\xi)=\frac{1}{2}\vec{w}^T\vec{w}+C\sum_{i=1}^N\xi_i$$
Under the constraints:
\begin{itemize}
    \item $d_i(\vec{w}^T\vec{x}_i+b)\geq1-\xi_i\forall i=1,\dots,N$
    \item $\xi_i\geq0\forall i=1,\dots,N$
\end{itemize}
C as a regularization hyper-parameter, a trade-off between empirical risk minimization and capacity term (VC-confidence) minimization:
\begin{itemize}
    \item Low C $\rightarrow$ many TR errors are allowed $\rightarrow$ possible underfitting;
    \item High C $\rightarrow$ no TR errors allowed $\rightarrow$ smaller margin, possible overfitting.
\end{itemize}

\subsubsection{Dual problem (soft margin)}
Given the training set $T=\left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N$ find the values of the Lagrangian multipliers 
$\left\{\alpha_i\right\}_{i=1}^N$ which maximize the objective function:
$$Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_j\vec{x}_i^Tx_j$$ 
Satisfying the constraints:
\begin{itemize}
    \item $\sum_{i=1}^N\alpha_id_i=0$
    \item $0\leq\alpha_i\leq C \forall i=1,\dots,N$
\end{itemize}
Note that $C$ is a user defined parameter.
\subsubsection{Kuhn-Tucker conditions (soft margin)}
The KT conditions are now defined as:
\begin{itemize}
    \item $\alpha_i(d_i(\vec{w}^T\vec{x}_i+b)+\xi_i-1)=0\forall i=1,\dots,N$;
    \item $\mu_i\xi_i=0\forall i=1,\dots,N$
\end{itemize}
Where the $\mu_i$ are Lagrange multipliers introduced to enforce the non-negativity of the slack variables (in the Lagrange primal function)

$$0<\alpha_i<C\rightarrow\xi_i=0 \text{(on the edge margin)}$$
$$\alpha_i=C\rightarrow\xi_i\geq0 \text{(inside the margin)}$$

\subsubsection{Solving the problem}
\begin{itemize}
    \item Solve the dual problem and obtain $\left\{\alpha_i\right\}_{i=1}^N$
    \item Find $\vec{w}_o$ and $b_o$ from $\left\{\alpha_{o,i}\right\}_{i=1}^N$
\end{itemize}
$$\vec{w}_o=\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i$$
$$b_o=d_j-\sum_{i=1}^N\alpha_{o,i}d_i\vec{x}_i^T\vec{x}_j \text{for a pattern $j$ s.t. $0<\alpha_j<C$}$$
Again, the non-zero Lagrangian multipliers correspond to support vectors.
We use this as before:
$$g(\vec{x})=\sum_{i=1}^N\alpha_{o.i}d_i\vec{x}_i^T\vec{x}+b_o$$
$$h(\vec{x})=sign(g(\vec{x}))$$

\subsection{Mapping to High-Dimensional feature space}
Idea: using a non-linear map for the data points in the input space leading to a high dimensional feature space, where they can be linearly separable.

\begin{enumerate}
    \item Cover's Theorem: the pattern are linearly separable with high probability in the feature space under such conditions.
    \item Finding the optimal hyperplane to separate the patterns in this new space.
\end{enumerate}
However, high dimensional feature spaces (see large basis function expansion) require control on the complexity of the problem; sometimes can lead to over-fitting or be computationally unfeasibile
$$\implies \text{Kernel approach}$$
\subsubsection{Solving the problem in the Feature Space}
Non linear function map
$$\phi : \mathcal{R}^{m_0}\longrightarrow\mathcal{R}^{m_1}$$
$$\vec{x}\longrightarrow\phi(\vec{x})$$
\begin{itemize}
    \item The problem is formulated as before, but the training set is now $T=\left\{\left(\phi(\vec{x}_i),d_i\right)\right\}_{i=1}^N$
    \item The hyperplane is then $\vec{w}^T\phi(\vec{x})+b=0$ and we can incorporate the bias in the weight vector:
    $$\begin{aligned}
        w(0)=b\\
        \phi_0(\vec{x})=1
    \end{aligned}\implies \phi(\vec{x})=\left(\phi_0(\vec{x})=1,\phi_1(\vec{x}),\dots,\phi_{m_1}(\vec{x})\right)^T$$
    So the decision surface equation is: $\vec{w}^T\phi(\vec{x})=0$
\end{itemize}
So we can follow same step as before, but considering the input transformation $\phi(\vec{x})$.
$$\vec{w}=\sum_{i=1}^N\alpha_id_i\phi_(\vec{x}_i)$$
Thus the hyperplane equation is:
$$\sum_{i=1}^N\alpha_id_i\phi^T(\vec{x}_i)\phi(\vec{x})=0$$
Note that the dot product is now in the feature space! Evaluating $\phi(\vec{x})$ could be intractable $\implies$ Kernel trick.

\subsubsection{Kernel trick}
We can use the Inner product Kernel Function $k$ to solve the problem without even evaluate $\phi(\vec{x})$ neither need to know the feature space itself!
$$k:\mathcal{R}^{m_0}\times\mathcal{R}^{m_0}\longrightarrow\mathcal{R}$$
$$k(\vec{x}_i,\vec{x})=\phi^T(\vec{x}_i)\phi(\vec{x})$$
$$k(\vec{x}_i,\vec{x})=k(\vec{x},\vec{x}_i)$$
Crucial point: the dot product in feature space is evaluated without considering the feature mapping and the feature space itself. We evaluate the dot product in terms of the input patterns.

We can then define the Kernel Matrix:
$$K=\begin{pmatrix}
k(\vec{x}_1,\vec{x}_1) & \dots & k(\vec{x}_1,\vec{x}_N)\\
k(\vec{x}_2,\vec{x}_1) & \dots & k(\vec{x}_2,\vec{x}_N)\\  
\dots & &\\
k(\vec{x}_N,\vec{x}_1) & \dots & k(\vec{x}_N,\vec{x}_N)   
\end{pmatrix}$$
$$K=\left\{k(\vec{x}_i,\vec{x}_j)\right\}_{(i,j)=1}^N$$
\subsubsection{Properties of Kernels}
\begin{itemize}
    \item Kernels gaining positive semi-definite kernel matrices (having non-negative eigenvalues of the kernel matrices) define a metric, so it is possible to compute the inner prdocut in a feature space through them.
    \item If $k_1$ and $k_2$ are kernels then the following are kernel functions aswell:
    $$\begin{aligned}
        &k_1(\vec{x},\vec{y})+k_2(\vec{x},\vec{y})\\
        &\alpha k_1(\vec{x},\vec{y})\forall\alpha\in\mathcal{R}_+\\
        &k_1(\vec{x},\vec{y})k_2(\vec{x},\vec{y})
    \end{aligned}$$
\end{itemize}
\subsubsection{Primal problem in feature space}
Given the training set $T=\left\{\left(\phi(x_i),d_i\right)\right\}_{i=1}^N$ find the optimal value of $\vec{w}$ which minimizes
the objective function:
$$\Psi(\vec{w},\xi)=\frac{1}{2}\vec{w}^T\vec{w}+C\sum_{i=1}^N\xi_i$$
under the constraints:
\begin{itemize}
    \item $d_i(\vec{w}^T\phi(x_i))\geq1-\xi_i\forall i=1,\dots,N$;
    \item $\xi_i\geq0\forall i=1,\dots,N$
\end{itemize}
Note that $\vec{w}$ is now in the feature space and thus its dimensionality could lead to an intractable problem.
\subsubsection{Dual problem in feature space}
Given the training set $T=\left\{\left(\phi(x_i),d_i\right)\right\}_{i=1}^N$ find the optimal values of $\left\{\alpha_i\right\}_{i=1}^N$ which maximize
the objective function:
$$Q(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i,j=1}^N\alpha_i\alpha_jd_id_jk(\vec{x}_i,\vec{x}_j)$$
satisfying the constraints:
\begin{itemize}
    \item $\sum_{i=1}^N\alpha_id_i=0$;
    \item $0\leq\alpha_i\leq C\forall i=1,\dots,N$
\end{itemize}
Remember $C$ is a user specified non-negative parameter (regularization parameter).

Remember also that the optimization problem solution gains a sparse solution in $\left\{\alpha\right\}_{i=1}^N$, as all the multipliers corresponding to a non-support vector are zeros.

\subsubsection{Some examples of Kernels}
\begin{itemize}
    \item Polynomial Learning Machine: $$k(\vec{x},\vec{x}_i)=(\vec{x}^T\vec{x}_i+1)^p$$
    where $p$ is a user specified parameter
    \item Radial Basis Function Net: $$k(\vec{x},\vec{x}_i)=e^{-\frac{1}{2\sigma^2}||\vec{x}-\vec{x}_i||^2}$$
    where $\sigma^2$ is a user specified parameter
    \item Two-layer perceptron: $$k(\vec{x},\vec{x}_i)=tanh(\beta_0\vec{x}^T\vec{x}_i+\beta_1)$$
    where $\beta_0>0$ and $\beta_1<0$ are user specified parameters. However this last example computes an inner product only for some choices for $\beta_0$ and $\beta_1$ (Mercer's theorem)
\end{itemize}

\section{SVM for non-linear regression}
Recall the regression problem:
$$T = \left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N \ \ \ \ \ \ d=f(\vec{x})+v$$
We estimate $d$ using a linear exapnsion of non-linear functions $\left\{\phi_j(\vec{x})\right\}_{j=0}^{m_1}$
$$y=h(\vec{x})=\vec{w}^T\phi(\vec{x})$$
Where: $$\begin{aligned}\vec{w}=\left(w(0)=b,w(1),\dots,w(m_1)\right)^T\\
\phi(\vec{x})=\left(\phi_0(\vec{x})=1, \phi_1(\vec{x}), \dots, \phi_{m_1}(\vec{x})\right)^T\end{aligned}$$
\subsection{$\epsilon$-insensitive (soft margin) loss}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{insensitive.png}
\end{figure}

\subsection{Formulating the optimization problem}
Introduce the non-negative slack variables $\xi_i'$ and $\xi_i$ $\forall i=1,\dots,N$:
$$-\xi_i'-\epsilon\leq d_i-\vec{w}^T\phi(\vec{x}_i)\leq\epsilon+\xi_i\forall i =1,\dots,N$$
This lead to the following constraints:
$$\forall i =1,\dots,N
\begin{cases}
    &d_i-\vec{w}^T\phi(\vec{x}_i)\leq\epsilon+\xi_i\\
    &\vec{w}^T\phi(\vec{x}_i)-d_i\leq\epsilon+\xi_i'\\
    &\xi_i\geq0\\
    &\xi_i'\geq0
\end{cases}$$

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{tube.png}
\end{figure}

\subsection{Primal problem}
Given the training set $T=\left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N$ find the optimal values of $\vec{w}$ s.t.
the following objective function is minimized:
$$\Psi(\vec{w},\xi,\xi')=\frac{1}{2}\vec{w}^T\vec{w}+C\sum_{i=1}^N\left(\xi_i+\xi_i'\right)$$
under the constraints
$$\forall i =1,\dots,N
\begin{cases}
    &d_i-\vec{w}^T\phi(\vec{x}_i)\leq\epsilon+\xi_i\\
    &\vec{w}^T\phi(\vec{x}_i)-d_i\leq\epsilon+\xi_i'\\
    &\xi_i\geq0\\
    &\xi_i'\geq0
\end{cases}$$

\subsection{Dual problem}
Given the training set $T=\left\{\left(\vec{x}_i,d_i\right)\right\}_{i=1}^N$ find the optimal values of $\left\{\alpha_i\right\}_{i=1}^N$ and
$\left\{\alpha_i'\right\}_{i=1}^N$ which maximize the following objective function:
$$Q(\alpha,\alpha')=\sum_{i=1}^Nd_i\left(\alpha_i-\alpha_i'\right)-\epsilon\sum_{i=1}^N\left(\alpha_i+\alpha_i'\right)-\frac{1}{2}\sum_{(i,j)=1}^N\left(\alpha_i-\alpha_i'\right)\left(\alpha_j-\alpha_j'\right)k(\vec{x}_i,\vec{x}_j)$$
under the constraints:
$$\forall i =1,\dots,N
\begin{cases}
    &d_i-\vec{w}^T\phi(\vec{x}_i)\leq\epsilon+\xi_i\\
    &\vec{w}^T\phi(\vec{x}_i)-d_i\leq\epsilon+\xi_i'\\
    &\xi_i\geq0\\
    &\xi_i'\geq0
\end{cases}$$
Remember that $C$ is a user specified parameter.
\subsection{Computing the weight vector}
Solving the dual problem we obtain the optimal values of the Lagrangian multipliers $\left\{\alpha_i\right\}_{i=1}^N$ and $\left\{\alpha_i'\right\}_{i=1}^N$

Then we can compute the optimal value of vector $\vec{w}$:
$$\vec{w}=\sum_{i=1}^N\left(\alpha_i-\alpha_i'\right)\phi(\vec{x}_i) = \sum_{i=1}^N\gamma_i\phi(\vec{x}_i)$$
So in this case support vectors correspond to non-zero values of $\gamma_i$.

\subsection{Computing the estimate}
Substituting in the estimate function:
$$h(\vec{x})=y=\vec{w}^T\phi(\vec{x})=\sum_{i=1}^N\gamma_i\phi^T(\vec{x}_i)\phi(\vec{x})=\sum_{i=1}^N\gamma_ik(\vec{x}_i,\vec{x})$$

\end{document}



